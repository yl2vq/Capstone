{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import datetime\n",
    "import numpy as np\n",
    "#from tensorflow.python.keras import layers\n",
    "#from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "#from keras.applications.resnet import ResNet50, ResNet101, preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet import ResNet101\n",
    "\n",
    "\n",
    "from keras.layers import TimeDistributed, Conv3D, Input, Flatten, Dense, Reshape\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "#References\n",
    "#Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH  = '/project/sds-capstone-aaai/met/DATA/Pytorch'\n",
    "# data \n",
    "IMAGE_SIZE    = (224, 224)\n",
    "#Note that the default input image size for this model is 299x299, instead of 224x224 as in the VGG16 and ResNet models.\n",
    "\n",
    "NUM_CLASSES   = 23\n",
    "#\n",
    "BATCH_SIZE    = 32  # try reducing batch size or freeze more layers if your GPU runs out of memory\n",
    "\n",
    "FREEZE_LAYERS = 2  # freeze the first this many layers for training\n",
    "NUM_EPOCHS    = 25\n",
    "# epoch \n",
    "WEIGHTS_FINAL = 'resnet50_25epochs.h5'\n",
    "# pretrained weights from image net - but the model will use the final weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing \n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   channel_shift_range=10,\n",
    "                                   horizontal_flip=True,\n",
    "                                   validation_split=0.3,\n",
    "                                   fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61131 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_batches = train_datagen.flow_from_directory(DATASET_PATH + '/train',\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  subset='training',\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26184 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_batches = train_datagen.flow_from_directory(DATASET_PATH + '/train',\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  subset='validation',\n",
    "                                                  shuffle=False,\n",
    "                                                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to augment validation data \n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9711 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = test_datagen.flow_from_directory(DATASET_PATH + '/valid',\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False,\n",
    "                                                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "Class #0 = architecture\n",
      "Class #1 = birds\n",
      "Class #2 = boats\n",
      "Class #3 = books\n",
      "Class #4 = bottles\n",
      "Class #5 = bowls\n",
      "Class #6 = buildings\n",
      "Class #7 = dogs\n",
      "Class #8 = flowers\n",
      "Class #9 = hieroglyphs\n",
      "Class #10 = horses\n",
      "Class #11 = houses\n",
      "Class #12 = landscapes\n",
      "Class #13 = leaves\n",
      "Class #14 = lions\n",
      "Class #15 = men\n",
      "Class #16 = ornament\n",
      "Class #17 = sculpture\n",
      "Class #18 = seals\n",
      "Class #19 = soldiers\n",
      "Class #20 = trees\n",
      "Class #21 = vases\n",
      "Class #22 = women\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# show class indices\n",
    "print('****************')\n",
    "for cls, idx in train_batches.class_indices.items():\n",
    "    print('Class #{} = {}'.format(idx, cls))\n",
    "print('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_imagenet_model = ResNet50(include_top=False, \n",
    "                                   weights='imagenet', \n",
    "                                   input_shape=(224, 224, 3))\n",
    "\n",
    "# freeze the layers\n",
    "for layer in resnet50_imagenet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "#Flatten output layer of Resnet\n",
    "flattened = tf.keras.layers.Flatten()(resnet50_imagenet_model.output)\n",
    "\n",
    "#Fully connected layer 1\n",
    "fc1 = tf.keras.layers.Dense(128, activation='relu', name=\"AddedDense1\")(flattened)\n",
    "\n",
    "#Fully connected layer, output layer\n",
    "fc2 = tf.keras.layers.Dense(23, activation='softmax', name=\"AddedDense2\")(fc1)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=resnet50_imagenet_model.input, outputs=fc2)\n",
    "\n",
    "\n",
    "    \n",
    "# set optimizer = ADAM \n",
    "# set loss criterion and metrics \n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 1 Accuracy 0.803\n",
    "\n",
    "\n",
    "#Top 5 Accuracy 0.953\n",
    "#Now, in the case of top-1 score, you check if the top class (the one having the highest probability) is the same as the target label.\n",
    "\n",
    "#In the case of top-5 score, you check if the target label is one of your top 5 predictions (the 5 ones with the highest probabilities).\n",
    "\n",
    "#In both cases, the top score is computed as the times a predicted label matched the target label, divided by the number of data-points evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2728 steps, validate for 303 steps\n",
      "Epoch 1/25\n",
      "2728/2728 [==============================] - 2980s 1s/step - loss: 2.0691 - accuracy: 0.3744 - val_loss: 3.2933 - val_accuracy: 0.2181\n",
      "Epoch 2/25\n",
      "2728/2728 [==============================] - 2994s 1s/step - loss: 1.7968 - accuracy: 0.4418 - val_loss: 3.5008 - val_accuracy: 0.2180\n",
      "Epoch 3/25\n",
      "2728/2728 [==============================] - 2983s 1s/step - loss: 1.7120 - accuracy: 0.4621 - val_loss: 3.7866 - val_accuracy: 0.2183\n",
      "Epoch 4/25\n",
      "1131/2728 [===========>..................] - ETA: 27:46 - loss: 1.6694 - accuracy: 0.4765"
     ]
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "#%%capture cap --no-stderr\n",
    "# beging training the model\n",
    "\n",
    "model.fit(train_batches,\n",
    "                        steps_per_epoch = train_batches.samples // BATCH_SIZE,\n",
    "                        validation_data = valid_batches,\n",
    "                        validation_steps = valid_batches.samples // BATCH_SIZE,\n",
    "                        epochs = NUM_EPOCHS)\n",
    "\n",
    "# save trained weights\n",
    "model.save(WEIGHTS_FINAL)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#with open('output.txt', 'w') as f:\n",
    "    #f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.history.history['accuracy']\n",
    "val_acc = model.history.history['val_accuracy']\n",
    "loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('resnet50_25epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "test_steps_per_epoch = numpy.math.ceil(test_batches.samples / test_batches.batch_size)\n",
    "\n",
    "predictions = model.predict_generator(test_batches, steps=test_steps_per_epoch)\n",
    "# Get most likely class\n",
    "predicted_classes = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = test_batches.classes\n",
    "class_labels = list(test_batches.class_indices.keys()) \n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_labels,output_dict=True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "#df.to_csv('classification_report_resnet50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "labels = class_labels\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_resnet50.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.1/Keras Py3.7",
   "language": "python",
   "name": "tensorflow210_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
